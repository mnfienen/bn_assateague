function [b,brmse,sk,n,msz,msr,nmse] = regr_xzw(X,z,w);% % [b,brmse,sk,n,msz,msr,nmse] = regr_xzw(X,z,w);% % general linear regression call% -nan- returned if data-data correlation matrix is badly conditioned%% Input%   X, nxm dependendant variables%   z, nx1 observations%   OPTIONAL w, nx1 weights (0 means observation has no influence)%% Output%   b, mx1 estimated parameters: z^ = X*b;%   brmse, mx1 estimated variances (root mean square error) of parameter(s)%      (confidence intervals assume gaussian white-noise dist., with bmse estimated variance)%   sk, the model skill%   n, the effective dof =(sum(w)/max(w))%   msz, variance of data%   msr, variance of residuals%   nmse, percent of white error input variance passed by weights% set condition testRCOND_TOL = 1e-6;% inputs[n,m] = size(X);nz = length(z);if nargin==2    w = ones(n,1);    nw = n;else    nw = length(w);endif(nz~=n | nw~=n | nw~=nz)    fprintf('X and z or w are different lengths\n')    returnend% init outputb = nan*ones(m,1);brmse = b;sk=nan;msz=nan;msr=nan;nmse = 1;% find valid data by summingid = find(isfinite([X,z,w]*ones(m+2,1))==1);if length(id)<2    fprintf('n<2 -- exiting\n')    returnend% number of dofn = sum(w(id))/max(w(id));% convert to weighted space (priestly p.315)z = (z).*w;X = X.*(repmat(w,1,m));% and compute covariancesXX = (X(id,:)'*X(id,:))/n;XZ = (z(id)'*X(id,:))/n;% solve the weighted least squares problemif(rcond(XX)>RCOND_TOL)    XX_inv = inv(XX);else    lasterr(sprintf('rcond(XX)=%g<%g\n', rcond(XX),RCOND_TOL));    return;    % lasterr('inverting with svd\n')     % XX_inv = svd_invP(XX,99);end% compute parametersb = XX_inv*XZ'; % model residualsmsz = (z(id)'*z(id))/n;msr = msz - (b')*XX*(b);sk = 1-msr/msz;% and perhaps we want all variance estimates% mse = XX_inv(1)*msr/(n-m);brmse = sqrt(diag(XX_inv)*msr/(n-m));% get normalized error, based on convoltionif (nargout==7)    % first comput regresion weights, assuming first input is all ones    bX = [X(id,:)*XX_inv(:,1)];     a = (bX(:,1).*w(id));    a = a/sum(a);    % sum of squared weights is normalized error: also, good est of dof    nmse = (a'*a);end%% Notes% get the right error estimate% read priestly, page 368: mse/msr ~ chi-sq(m)/(N-m)% here is argument:% var_observed = var_model + var_residual = var_true + var_noise% and% var_model = var_true + var_artificial%    var_true is variance of perfect model %    var_noise is additive white noise%    var_artificial is due to random correlations, i.e., aliased% a linear model will pick up this much of noise% var_a = (m/N) var_n, our extra bit of information% Thus,% var_n = var_r + var_a = var_r + (m/N) var_n = var_r /(1-m/N) %       = var_r*N/(N-m), expected value% and% var_a = var_r (m/N)/(1-m/N)%       = (m/(N-m)) var_r% thus: mse_of_model = ( msr*m/(sumw-m) );% this is plausible error in model, assumed uniform over range of data% BUT, we are interested in error of estimate of b(1), the value at x=0% See priestly page on regression models, which states that % b are normally dist. around b_true, with var(b) = var_noise*diag(XX_inv)% msn = msr*sumw/(sumw-m);% mse_of_b(1) = XX_inv(1)*msn/N; -- Look to the F-dist to explain ratio of variances% after testing synthetic examples, conclude that this is robust % even leaving large mean values in!